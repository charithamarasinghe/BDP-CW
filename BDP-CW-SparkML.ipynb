{
  "metadata": {
    "name": "CW SparkML",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Preprocessing (Testing)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\nimport copy\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import *\n\n\nschema \u003d StructType() \\\n      .add(\"date\",StringType(),True) \\\n      .add(\"home_team\",StringType(),True) \\\n      .add(\"away_team\",StringType(),True) \\\n      .add(\"home_score\",IntegerType(),True) \\\n      .add(\"away_score\",IntegerType(),True) \\\n      .add(\"tournament\",StringType(),True) \\\n      .add(\"city\",StringType(),True) \\\n      .add(\"country\",StringType(),True) \\\n      .add(\"neutral\",StringType(),True)\n\ndata \u003d spark.read.format(\"csv\").option(\"header\", True).schema(schema).load(\"/data/mllib/results.csv\")\ndata \u003d data.limit(5)\n\ndata \u003d data.filter(data[\"home_score\"] !\u003d data[\"away_score\"])\ndata \u003d data.selectExpr(\"home_team as team\", \"away_team as rival\", \"home_score as team_score\", \"away_score as rival_score\", \"city\")\n\nteam_reverted_data \u003d spark.read.format(\"csv\").option(\"header\", True).schema(schema).load(\"/data/mllib/results.csv\")\nteam_reverted_data \u003d team_reverted_data.limit(5)\n\nteam_reverted_data \u003d team_reverted_data.filter(team_reverted_data[\"home_score\"] !\u003d team_reverted_data[\"away_score\"])\nteam_reverted_data \u003d team_reverted_data.selectExpr( \"away_team as team\", \"home_team as rival\", \"away_score as team_score\", \"home_score as rival_score\", \"city\")\n\ncombined_data \u003d data.union(team_reverted_data)\ncombined_data \u003d combined_data.withColumn(\u0027win\u0027, F.when(F.col(\"team_score\") \u003e F.col(\"rival_score\"), 1).otherwise(0))\ncombined_data \u003d combined_data.select([\"team\", \"rival\", \"city\", \"win\"])\n\ncombined_data.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Prediction pipeline  (Testing)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoderEstimator, IndexToString\nfrom pyspark.ml.classification import RandomForestClassifier\n\ndata \u003d spark.read.option(\"header\",True) \\\n     .csv(\"/data/mllib/results.csv\")\n     \ndata \u003d data.limit(500)\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) \u003d data.randomSplit([0.7, 0.3])\n\n# define stage 1: transform the column feature_1 to numeric\nstage_1 \u003d StringIndexer(inputCol\u003d \u0027home_team\u0027, outputCol\u003d \u0027feature_1_index\u0027)\n# define stage 2: transform the column feature_2 to numeric\nstage_2 \u003d StringIndexer(inputCol\u003d \u0027away_team\u0027, outputCol\u003d \u0027feature_2_index\u0027)\n# define stage 2: transform the column feature_3 to numeric\nstage_3 \u003d StringIndexer(inputCol\u003d \u0027city\u0027, outputCol\u003d \u0027feature_3_index\u0027)\n\nlabelIndexer \u003d StringIndexer(inputCol\u003d\"neutral\", outputCol\u003d\"indexedLabel\")\n\n# define stage 3: one hot encode the numeric versions of feature 1,  2 and 3 generated from stage 1, 2 and stage 3\nstage_4 \u003d OneHotEncoderEstimator(inputCols\u003d[stage_1.getOutputCol(), stage_2.getOutputCol(), stage_3.getOutputCol()], \n                                 outputCols\u003d [\u0027feature_1_encoded\u0027, \u0027feature_2_encoded\u0027, \u0027feature_3_encoded\u0027])\n                                 \n# define stage 4: create a vector of all the features required to train the logistic regression model \nstage_5 \u003d VectorAssembler(inputCols\u003d[\u0027feature_1_encoded\u0027, \u0027feature_2_encoded\u0027, \u0027feature_3_encoded\u0027],\n                          outputCol\u003d\u0027features\u0027)\n                          \n# define stage 5: logistic regression model                          \nstage_6 \u003d RandomForestClassifier(featuresCol\u003d\u0027features\u0027,labelCol\u003d\u0027indexedLabel\u0027, numTrees\u003d10)\n\n\n\n# setup the pipeline\ntraining_pipeline \u003d Pipeline(stages\u003d [labelIndexer, stage_1, stage_2, stage_3, stage_4, stage_5, stage_6])\n\n# fit the pipeline for the trainind data\nmodel \u003d training_pipeline.fit(trainingData)\n\n# transform the data\nprediction \u003d model.transform(testData)\n\n# view some of the columns generated\npredictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator \u003d MulticlassClassificationEvaluator(\n    labelCol\u003d\"indexedLabel\", predictionCol\u003d\"prediction\", metricName\u003d\"accuracy\")\naccuracy \u003d evaluator.evaluate(predictions)\nprint(\"Test Error \u003d %g\" % (1.0 - accuracy))\n\nrfModel \u003d model.stages[2]\nprint(rfModel)  # summary only"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Complete Pipeline"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\nimport copy\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import *\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoderEstimator, IndexToString\nfrom pyspark.ml.classification import RandomForestClassifier\n\n\nschema \u003d StructType() \\\n      .add(\"date\",StringType(),True) \\\n      .add(\"home_team\",StringType(),True) \\\n      .add(\"away_team\",StringType(),True) \\\n      .add(\"home_score\",IntegerType(),True) \\\n      .add(\"away_score\",IntegerType(),True) \\\n      .add(\"tournament\",StringType(),True) \\\n      .add(\"city\",StringType(),True) \\\n      .add(\"country\",StringType(),True) \\\n      .add(\"neutral\",StringType(),True)\n\ndata \u003d spark.read.format(\"csv\").option(\"header\", True).schema(schema).load(\"/data/mllib/results.csv\")\ndata \u003d data.filter(data[\"home_score\"] !\u003d data[\"away_score\"])\ndata \u003d data.selectExpr(\"home_team as team\", \"away_team as rival\", \"home_score as team_score\", \"away_score as rival_score\", \"city\")\n\nteam_reverted_data \u003d spark.read.format(\"csv\").option(\"header\", True).schema(schema).load(\"/data/mllib/results.csv\")\nteam_reverted_data \u003d team_reverted_data.filter(team_reverted_data[\"home_score\"] !\u003d team_reverted_data[\"away_score\"])\nteam_reverted_data \u003d team_reverted_data.selectExpr( \"away_team as team\", \"home_team as rival\", \"away_score as team_score\", \"home_score as rival_score\", \"city\")\n\ncombined_data \u003d data.union(team_reverted_data)\ncombined_data \u003d combined_data.withColumn(\u0027win\u0027, F.when(F.col(\"team_score\") \u003e F.col(\"rival_score\"), 1).otherwise(0))\ncombined_data \u003d combined_data.select([\"team\", \"rival\", \"city\", \"win\"])\n\ncombined_data \u003d combined_data.limit(1000)\n# Split the data into training and test sets (30% held out for testing)\n(trainingData, testData) \u003d combined_data.randomSplit(weights\u003d[0.7, 0.3], seed\u003d1234)\n\n# define stage 1: transform the column feature_1 to numeric\nstage_1 \u003d StringIndexer(inputCol\u003d \u0027team\u0027, outputCol\u003d \u0027feature_1_index\u0027).setHandleInvalid(\"keep\")\n# define stage 2: transform the column feature_2 to numeric\nstage_2 \u003d StringIndexer(inputCol\u003d \u0027rival\u0027, outputCol\u003d \u0027feature_2_index\u0027).setHandleInvalid(\"keep\")\n# define stage 2: transform the column feature_3 to numeric\nstage_3 \u003d StringIndexer(inputCol\u003d \u0027city\u0027, outputCol\u003d \u0027feature_3_index\u0027).setHandleInvalid(\"keep\")\n\n\n# define stage 3: one hot encode the numeric versions of feature 1,  2 and 3 generated from stage 1, 2 and stage 3\nstage_4 \u003d OneHotEncoderEstimator(inputCols\u003d[stage_1.getOutputCol(), stage_2.getOutputCol(), stage_3.getOutputCol()], \n                                 outputCols\u003d [\u0027feature_1_encoded\u0027, \u0027feature_2_encoded\u0027, \u0027feature_3_encoded\u0027])\n                                 \n# define stage 4: create a vector of all the features required to train the logistic regression model \nstage_5 \u003d VectorAssembler(inputCols\u003d[\u0027feature_1_encoded\u0027, \u0027feature_2_encoded\u0027, \u0027feature_3_encoded\u0027],\n                          outputCol\u003d\u0027features\u0027)\n                          \n# define stage 5: logistic regression model                          \nstage_6 \u003d RandomForestClassifier(featuresCol\u003d\u0027features\u0027,labelCol\u003d\u0027win\u0027, numTrees\u003d10)\n\n\n\n# setup the pipeline\ntraining_pipeline \u003d Pipeline(stages\u003d [stage_1, stage_2, stage_3, stage_4, stage_5, stage_6])\n\n# fit the pipeline for the trainind data\nmodel \u003d training_pipeline.fit(trainingData)\n\n# transform the data\nprediction \u003d model.transform(testData)\n\n# view some of the columns generated\nprediction.select([\"prediction\", \"win\", \"features\"])\n\n# Select (prediction, true label) and compute test error\nevaluator \u003d MulticlassClassificationEvaluator(labelCol\u003d\"win\", predictionCol\u003d\"prediction\", metricName\u003d\"accuracy\")\naccuracy \u003d evaluator.evaluate(prediction)\nprint(\"Accuracy \u003d %g\" % accuracy)\nprint(\"Test Error \u003d %g\" % (1.0 - accuracy))\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}